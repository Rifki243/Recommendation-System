# -*- coding: utf-8 -*-
"""MLT Proyek Akhir - Recomendation System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D9uBVIAwTYm_f6vg6RaMtKoaOgtDBF0W

# Book Recommendation System and Prediction

## Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')

"""## Data Loading and Exploration Data"""

# Upload semua file CSV yang kamu butuhkan
uploaded = files.upload()

# Load masing-masing file ke dalam DataFrame
df_user = pd.read_csv('Users.csv')
df_books = pd.read_csv('Books.csv')
df_rating = pd.read_csv('Ratings.csv')

print(df_user.head())
print("-"*80)
print(df_books.head())
print("-"*80)
print(df_rating.head())

"""### Statistik Deskriptif"""

df_user.info()
print("-"*50)
df_user.describe()

df_books.info()
print("-"*80)
df_books.describe()

df_rating.info()
print("-"*50)
df_rating.describe()

"""### Mengecek Missing Value"""

print(f"Dataset User\n{df_user.isnull().sum()}")
print("-" * 25)
print(f"Dataset Books\n{df_books.isnull().sum()}")
print("-" * 25)
print(f"Dataset Rating\n{df_rating.isnull().sum()}")

"""## Data Preparation

### Exploratory Data Analysis (EDA)

#### Mengubah Nama Kolom dan Tipe Data
"""

# Mengubah nama kolom
df_user.rename(columns={'User-ID':'userID'},inplace=True)
df_books.rename(columns={'Book-Title':'Title','Book-Author':'Author','Year-Of-Publication':'Year'},inplace=True)
df_rating.rename(columns={'Book-Rating':'Rating','User-ID':'userID',},inplace=True)

# Mengubah Tipe Data
df_user['Age'] = df_user['Age'].astype('Int64')

"""#### Dataset Buku `df_books`"""

df_books.info()

"""**1. Menghapus Kolom Yang Tidak Diperlukan**"""

df_books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'],axis=1, inplace=True)
df_books.head()

"""**2. Melihat Data Anomali Pada Kolom `Year`**"""

df_books['Year'].unique()

"""Insight

Terdapat kesalahan pada pengisian tahun penerbitan.
"""

df_books[df_books['Year'].isin(['DK Publishing Inc', 'Gallimard'])]

"""Insight


Ternyata pada kolom `Title` terdapat nama Author yang seharusnya berada pada kolom `Author` dan begitu juga dengan kolom `Year` dan `Publisher`.
"""

# Data perbaikan
book_fixes = [
    {
        'ISBN': '0789466953',
        'Year': 2000,
        'Author': 'James Buckley',
        'Publisher': 'DK Publishing Inc',
        'Title': 'DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)'
    },
    {
        'ISBN': '078946697X',
        'Year': 2000,
        'Author': 'Michael Teitelbaum',
        'Publisher': 'DK Publishing Inc',
        'Title': 'DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)'
    },
    {
        'ISBN': '2070426769',
        'Year': 2003,
        'Author': 'Jean-Marie Gustave Le ClÃ©zio',
        'Publisher': 'Gallimard',
        'Title': "Peuple du ciel, suivi de 'Les Bergers"
    }
]

# Perulangan untuk update
for book in book_fixes:
    df_books.loc[df_books.ISBN == book['ISBN'], ['Year', 'Author', 'Publisher', 'Title']] = [
        book['Year'],
        book['Author'],
        book['Publisher'],
        book['Title']
    ]

# Cek data perbaikan
df_books.loc[df_books.ISBN.isin(['0789466953', '078946697X', '2070426769']), :]

"""**3. Mengecek Missing Value**"""

df_books.isnull().sum()

df_books[df_books['Author'].isnull() | df_books['Publisher'].isnull()]

"""Insight

Ternyata ada nilai null (`NaN`) pada kolom `Author` dan `Publisher`.
"""

df_books = df_books.dropna(subset=['Author', 'Publisher'])
df_books.isnull().sum()

"""**4. Mengubah Tipe Data Kolom `Year`**"""

df_books['Year'] = pd.to_numeric(df_books['Year'], errors='coerce')
# df_books['Year'] = df_books['Year'].astype('Int64')
df_books['Year'].sort_values().unique()

"""**5. Menyortir Buku Berdasarkan Tahun Terbit**"""

df_books[(df_books['Year'] >= 1950) & (df_books['Year'] <= 2025) & (df_books['Year'] != 0)]

df_books[df_books['Year'] == 0]

"""Insight

Terdapat angka 0 pada tahun terbit yang dimana ini adalah sebuah kesalahan yang terjadi dari sisi penginputan ataupun nilai placeholder yang diisi dengan angka `0` bukan `NaN`.
"""

df_books = df_books.loc[(df_books['Year'] >= 1950) & (df_books['Year'] <= 2025) & (df_books['Year'] != 0)]
df_books['Year'].sort_values().unique()

"""#### Dataset Pengguna `df_user`"""

df_user.info()

"""**1. Mengecek Missing Value**"""

df_user.isnull().sum()

"""Insight

Terdapat banyak nilai null pada kolom `Age`.
"""

df_user[df_user['Age'].isnull()]

df_user = df_user.dropna(subset=['Age'])
df_user.isnull().sum()

"""**2. Menyortir Umur Pengguna**"""

df_user['Age'].sort_values().unique()

"""Insight

Terdapat umur yang tidak masuk akal, yaitu minimal 0 dan maksimal 244, pada kasus rekomendasi buku ini saya akan menghapus nilai 0 dan membuat rentang umur dari 10 sampai 70 tahun.
"""

df_user = df_user.loc[(df_user['Age'] >= 10) & (df_user['Age'] <= 70)]
df_user['Age'].sort_values().unique()

"""#### Dataset Peringkat `df_rating`"""

df_rating.info()

"""**1. Mengecek Missing Value**"""

df_rating.isnull().sum()

"""**2. Mengecek Duplikasi Data**"""

df_rating.duplicated().sum()

"""Insight

Pada datset rating terlihat bersih dengan tidak adanya data kosong dan duplikat.

### Combined Data

**Menggabungkan dataset rating dan dataset buku**
"""

df_combined = pd.merge(df_rating, df_books, on='ISBN', how='left')
df_combined

"""**Mengcopy dataset gabungan dan membuat dataset baru**"""

df_preparation = df_combined.copy()

"""**Mengecek missing value**"""

df_preparation.isnull().sum()

"""Insight

Terlihat pada dataset gabungan banyak kolom yang nilainya kosong, itu terjadi karena ketika proses penggabungan dan kolom `ISBN` sebagai acuan ada yang tidak sesuai dengan ISBN yang ada pada dataset buku.

**Menghapus missing value dengan code `.dropna()`**
"""

df_preparation = df_preparation.dropna(subset=['Title', 'Author', 'Year'])
df_preparation.isnull().sum()

"""**Mengecek data duplikat**"""

df_preparation['ISBN'].duplicated().sum()

"""Insight

Karena proses penggabungan pasti terdapat data yang duplikat, maka dari itu saya akan menghapus data duplikat.

**Menghapus data duplikat dengan code `.drop_duplicates()`**
"""

df_preparation = df_preparation.drop_duplicates(subset='ISBN')
df_preparation['ISBN'].duplicated().sum()

df_preparation.info()

"""**Mengubah tipe data pada kolom `Year`**"""

df_preparation['Year'] = df_preparation['Year'].astype('Int64')
df_preparation.info()

"""Insight

Sebelum proses penggabungan kolom `Year` memang sudah berbentuk `Int64`, namun karena proses penggabungan dia berubah menjadi `float6`4, maka dari itu saya akan ganti menjadi tipe data `Int64`.
"""

df_preparation

"""### Data Splitting"""

# Encoding userID dan ISBN ke angka
user_ids = df_rating['userID'].unique().tolist()
u_enc = {x: i for i, x in enumerate(user_ids)}
u_dec = {i: x for i, x in enumerate(user_ids)}

book_ids = df_rating['ISBN'].unique().tolist()
b_enc = {x: i for i, x in enumerate(book_ids)}
b_dec = {i: x for i, x in enumerate(book_ids)}

# Map kolom user dan book hasil encoding
df_rating['user'] = df_rating['userID'].map(u_enc)
df_rating['book'] = df_rating['ISBN'].map(b_enc)

# Jumlah user dan buku
num_u = len(u_enc)
num_b = len(b_enc)

# Ubah rating ke float, lalu normalisasi ke rentang 0-1
df_rating['Rating'] = df_rating['Rating'].astype(float)
min_r = df_rating['Rating'].min()
max_r = df_rating['Rating'].max()
df_rating['rating_norm'] = (df_rating['Rating'] - min_r) / (max_r - min_r)

# Acak data secara random dengan seed agar reproducible
df_rating = df_rating.sample(frac=1, random_state=42)

# Siapkan fitur input (user dan book) dan target (rating dinormalisasi)
x = df_rating[['user', 'book']].values
y = df_rating['rating_norm'].values

# Split data 80% train dan 20% validation
train_idx = int(0.8 * len(df_rating))
x_train, x_val = x[:train_idx], x[train_idx:]
y_train, y_val = y[:train_idx], y[train_idx:]

print(f"Jumlah user: {num_u}, Jumlah buku: {num_b}")
print(f"Data train shape: {x_train.shape}, Data val shape: {x_val.shape}")

"""## Modeling & Results

**Create Model RecommenderNet & DNNRecommender**
"""

class RecommenderNet(keras.Model):
    def __init__(self, num_users, num_books, embedding_size=50, **kwargs):
        super().__init__(**kwargs)
        self.user_embedding = layers.Embedding(num_users, embedding_size,
                                               embeddings_initializer='he_normal',
                                               embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.user_bias = layers.Embedding(num_users, 1)
        self.book_embedding = layers.Embedding(num_books, embedding_size,
                                               embeddings_initializer='he_normal',
                                               embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.book_bias = layers.Embedding(num_books, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        book_vector = self.book_embedding(inputs[:, 1])
        book_bias = self.book_bias(inputs[:, 1])
        dot = tf.reduce_sum(user_vector * book_vector, axis=1, keepdims=True)
        x = dot + user_bias + book_bias
        return tf.squeeze(x, axis=1)

class DNNRecommender(keras.Model):
    def __init__(self, num_users, num_books, embedding_size=50):
        super().__init__()
        self.user_embedding = layers.Embedding(num_users, embedding_size)
        self.book_embedding = layers.Embedding(num_books, embedding_size)
        self.concat = layers.Concatenate()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1)

    def call(self, inputs):
        user_vec = self.user_embedding(inputs[:, 0])
        book_vec = self.book_embedding(inputs[:, 1])
        x = self.concat([user_vec, book_vec])
        x = self.dense1(x)
        x = self.dense2(x)
        return self.output_layer(x)

"""**Training Process**"""

models = {
    "RecommenderNet": RecommenderNet(num_users=num_u, num_books=num_b, embedding_size=50),
    "DNNRecommender": DNNRecommender(num_users=num_u, num_books=num_b, embedding_size=50)
}

histories = {}

for model_name, model in models.items():
    print(f"Training model: {model_name}")

    model.compile(
        loss=tf.keras.losses.MeanSquaredError(),
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )

    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True,
        verbose=1
    )

    model_checkpoint = ModelCheckpoint(
        f'best_model_{model_name}.h5',
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    )

    history = model.fit(
        x=x_train,
        y=y_train,
        batch_size=8192,
        epochs=20,
        validation_data=(x_val, y_val),
        callbacks=[early_stopping, model_checkpoint]
    )

    histories[model_name] = history

"""**Create Top-N Recommendation Books**"""

# Persiapan data buku dan rating dari df_preparation
df_books_clean = df_preparation[['ISBN', 'Title', 'Author', 'Publisher', 'Year']].drop_duplicates()
df_rating_clean = df_preparation[['userID', 'ISBN', 'Rating']]

# Pilih user secara acak
sample_user_id = df_rating_clean['userID'].sample(1).iloc[0]

# Ambil semua rating user
user_books = df_rating_clean[df_rating_clean['userID'] == sample_user_id]

# Gabungkan dengan info buku (df_books_clean)
user_books = user_books.merge(df_books_clean, on='ISBN')

# Urutkan berdasarkan rating tertinggi
user_books_sorted = user_books.sort_values(by='Rating', ascending=False)

# Ambil 5 buku terbaik
top_read_books = user_books_sorted.head(5)

# Daftar ISBN yang sudah dibaca
books_read = user_books['ISBN'].tolist()

# Cari buku yang belum dibaca
books_unread = df_books_clean[~df_books_clean['ISBN'].isin(books_read)]['ISBN']

# Encode ISBN (hanya yang ada di book encoder)
books_unread_encoded = [b_enc[isbn] for isbn in books_unread if isbn in b_enc]

# Encode user ID
user_encoded = u_enc[sample_user_id]

# Siapkan data prediksi
user_book_array = np.array([[user_encoded, book] for book in books_unread_encoded])

# Prediksi rating
predicted_ratings = model.predict(user_book_array, verbose=0).flatten()

# Ambil top 10 rekomendasi
top_indices = predicted_ratings.argsort()[-10:][::-1]
top_books_isbn = [b_dec[books_unread_encoded[i]] for i in top_indices]

# Loop untuk setiap model
for model_name, model in models.items():
    predicted_ratings = model.predict(user_book_array, verbose=0).flatten()
    top_indices = predicted_ratings.argsort()[-10:][::-1]
    top_books_isbn = [b_dec[books_unread_encoded[i]] for i in top_indices]

    print(f'\n===== Rekomendasi untuk User ID: {sample_user_id} - Model: {model_name} =====\n')

    print('Buku yang sudah dibaca (top 5 berdasarkan rating tertinggi):')
    for i, (_, row) in enumerate(top_read_books.iterrows(), 1):
        print(f"{i}. {row['Author']} : {row['Title']} (Rating: {row['Rating']})")

    print('\nRekomendasi buku baru:')
    for i, isbn in enumerate(top_books_isbn, 1):
        matched_books = df_books_clean[df_books_clean['ISBN'] == isbn]
        if not matched_books.empty:
            info = matched_books.iloc[0]
            print(f"{i}. {info['Author']} : {info['Title']}")
        else:
            print(f"{i}. ISBN {isbn} tidak ditemukan di dataset buku")

"""Insight

Untuk menguji kemampuan sistem rekomendasi yang telah dikembangkan, dilakukan simulasi inferensi dengan memilih satu pengguna secara acak dari data rating, yaitu pengguna dengan userID tertentu. Selanjutnya, seluruh data rating dari pengguna tersebut diambil dan digabungkan dengan informasi detail buku seperti judul, penulis, dan penerbit. Buku-buku yang telah dibaca oleh pengguna diurutkan berdasarkan nilai rating tertinggi, dan lima buku teratas diidentifikasi sebagai buku yang paling disukai oleh pengguna tersebut. ISBN dari buku-buku yang sudah pernah dibaca kemudian disimpan agar dapat dibedakan dari buku yang belum dibaca.
"""

top5_books = df_preparation[df_preparation['userID'] == sample_user_id].sort_values(by='Rating', ascending=False).head(5)
top5_books[['userID', 'ISBN', 'Title', 'Author', 'Rating']]

"""Insight

Dari hasil rekomendasi dua model, yaitu RecommenderNet dan DNNRecommender, terhadap User ID: 254, diperoleh beberapa insight menarik. Berdasarkan lima buku dengan rating tertinggi yang telah dibaca oleh pengguna, terlihat bahwa pengguna memiliki preferensi kuat terhadap fiksi fantasi, kisah imajinatif, dan narasi karakter-driven. Buku-buku seperti Neverwhere oleh Neil Gaiman dan The Subtle Knife oleh Philip Pullman menunjukkan kecenderungan pengguna terhadap dunia alternatif, petualangan, dan cerita dengan kedalaman karakter serta elemen magis.

**RecommenderNet Model**

Model RecommenderNet merekomendasikan buku-buku dari penulis populer seperti J.K. Rowling, Neil Gaiman, dan Michael Crichton. Genre yang mendominasi adalah fantasi, petualangan, dan fiksi ilmiah populer. Misalnya, seri Harry Potter mendominasi rekomendasi, diikuti oleh karya seperti American Gods dan A Wrinkle in Time.

Hal ini menunjukkan bahwa RecommenderNet cenderung mengandalkan popularitas global dan korelasi genre umum untuk menyusun rekomendasi. Model ini cocok untuk pengguna yang ingin menjelajahi buku-buku mainstream dari genre yang mereka sukai. Namun, masih ditemukan duplikasi dalam daftar (misalnya Harry Potter and the Chamber of Secrets muncul dua kali, begitu juga A Wrinkle in Time), yang menunjukkan perlunya perbaikan dalam deduplikasi hasil.

**DNNRecommender Model**

Model DNNRecommender menampilkan pendekatan yang lebih personal dan beragam. Selain menyarankan beberapa buku dari seri Harry Potter, model ini juga merekomendasikan buku dengan nilai literatur tinggi dan unik, seperti To Kill a Mockingbird oleh Harper Lee, Life of Pi oleh Yann Martel, serta puisi anak Falling Up oleh Shel Silverstein.

Menariknya, model ini juga menyarankan buku Free karya Paul Vincent, yang tidak umum ditemukan di daftar rekomendasi populer, mengindikasikan bahwa model ini memperhatikan kecenderungan pengguna terhadap tema yang reflektif dan puitis. Meski demikian, masih terdapat duplikasi judul dalam seri yang sama (Harry Potter), yang menandakan perlunya peningkatan dalam aspek diversifikasi hasil.

Kesimpulan

Secara umum, kedua model menunjukkan kelebihannya masing-masing:

- RecommenderNet unggul dalam memberikan rekomendasi mainstream dan populer, cocok bagi pengguna yang ingin eksplorasi aman dalam genre favorit mereka.

- DNNRecommender memberikan rekomendasi yang lebih personal dan beragam, namun memerlukan peningkatan dalam deduplikasi dan cakupan genre.

## Evaluation
"""

for model_name, history in histories.items():
    plt.plot(history.history['root_mean_squared_error'], label=f'{model_name} Train RMSE')
    plt.plot(history.history['val_root_mean_squared_error'], label=f'{model_name} Val RMSE')

plt.title('RMSE Comparison per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Root Mean Squared Error')
plt.legend()
plt.show()

"""Insight

Berdasarkan hasil training model RecommenderNet dan DNNRecommender, keduanya menunjukkan penurunan nilai loss dan root mean squared error (RMSE) yang signifikan pada epoch-epoch awal. Ini mengindikasikan bahwa model mampu belajar dengan cepat pada tahap awal pelatihan.

Untuk model RecommenderNet, performa terbaik pada data validasi tercapai pada epoch ke-5, dengan val_loss sebesar 0.15574 dan val_RMSE sebesar 0.3835. Setelah epoch ke-5, nilai validation loss mulai meningkat, sementara training loss terus membaik hingga epoch ke-8. Hal ini menunjukkan bahwa model mulai mengalami overfitting, yaitu terlalu menyesuaikan diri terhadap data latih dan kehilangan kemampuan generalisasinya terhadap data baru. Mekanisme EarlyStopping bekerja dengan baik dengan menghentikan pelatihan pada epoch ke-8 dan mengembalikan bobot model ke kondisi terbaik dari epoch ke-5.

Sementara itu, untuk model DNNRecommender, performa terbaik dicapai sangat awal, yaitu pada epoch pertama, dengan val_loss sebesar 0.11364 dan val_RMSE sebesar 0.3371. Setelahnya, validation loss meningkat secara konsisten hingga epoch ke-4, menunjukkan bahwa model mengalami overfitting sangat cepat dan tidak memperoleh manfaat dari pelatihan lebih lama. EarlyStopping menghentikan pelatihan dan mengembalikan bobot model ke kondisi pada epoch pertama.

Secara keseluruhan, kedua model menunjukkan kemampuan belajar yang baik di awal, dan penggunaan pendekatan EarlyStopping terbukti efektif untuk menjaga performa model dengan mencegah overfitting. Namun, DNNRecommender cenderung lebih sensitif terhadap overfitting dan mungkin memerlukan regularisasi tambahan atau penyesuaian arsitektur agar dapat mempertahankan performa pada data validasi.
"""